{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "illegal-number",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get('https://www.naver.com')\n",
    "bs = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "\n",
    "# find로 접근하기\n",
    "lists = bs.find_all('li', {'class': 'realtime_item'})\n",
    "\n",
    "#print(lists)\n",
    "for li in lists:\n",
    "    title = li.find('span', {'class': 'keyword'}).text\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "automatic-child",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get('https://www.naver.com')\n",
    "bs = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# select로 접근하기\n",
    "bs.select('li.keyword')\n",
    "\n",
    "for li in lists:\n",
    "    title = li.find('span', {'class': 'keyword'})[0].text  # select는 list를 반환\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lxml이 html.parser보다 빠르다.\n",
    "# lxml 설치해야 함\n",
    "\n",
    "def time_function(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end_time = time.time() - start_time\n",
    "        print(f'{f.__name__} {args[1]} time {end_time}')\n",
    "        return result\n",
    "    return wrapper\n",
    "  \n",
    "@time_function\n",
    "def r_find_all(url, parser):\n",
    "    r = requests.get(url)\n",
    "    bs = BeautifulSoup(r.text, parser)\n",
    "    lists = bs.find_all('li', {'class': 'realtime_item'})\n",
    "    \n",
    "    titles = []\n",
    "    for li in lists:\n",
    "    title = li.find('span', {'class': 'keyword'}).text\n",
    "    print(title)\n",
    "    \n",
    "@time_function\n",
    "def r_select(url, parser):\n",
    "    r = requests.get(url)\n",
    "    bs = BeautifulSoup(r.text, parser)\n",
    "    lists = bs.select('li.realtime_item')\n",
    "    \n",
    "    titles = []\n",
    "    for li in lists:\n",
    "    title = li.find('span', {'class': 'keyword'})[0].text\n",
    "    print(title)\n",
    "\n",
    "url = 'https://www.naver.com'\n",
    "r_find_all(url, 'html.parser')\n",
    "r_select(url, 'html.parser')\n",
    "\n",
    "r_find_all(url, 'lxml')\n",
    "r_select(url, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-japanese",
   "metadata": {},
   "source": [
    "### 2020년 1월 17일부로 네이버 실시간 검색이 ajax통신으로 변경 됨.\n",
    "따라서, 위코드로 lists를 찍어보면 []을 가져옴.\n",
    "\n",
    "수정된 코드는 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "logical-mounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 달이 뜨는 강 인물관계도\n",
      "2 이현이\n",
      "3 딩크족\n",
      "4 차웅기\n",
      "5 백기완\n",
      "6 바보온달과 평강공주\n",
      "7 고민정\n",
      "8 박정수\n",
      "9 반찬김치달인\n",
      "10 김희갑\n",
      "11 히피펌\n",
      "12 이상화\n",
      "13 조항조\n",
      "14 미국주식 휴장일\n",
      "15 안싸우면 다행이야\n",
      "16 삥미용실\n",
      "17 강하늘\n",
      "18 박솔미\n",
      "19 에볼라 바이러스\n",
      "20 이하늘\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 아래 주소가 메인페이지 내부에서 호출되는 실시간 검색어 데이터를 넘겨주는 주소\n",
    "# requests.get(\"주소\").json() 을 하면 데이터를 json 형태로 받아올 수 있습니다.\n",
    "# 아래 주소를 직접 브라우저에서 접속해보시기 바랍니다.\n",
    "json = requests.get('https://www.naver.com/srchrank?frm=main').json()\n",
    "\n",
    "# json 데이터에서 \"data\" 항목의 값을 추출\n",
    "ranks = json.get(\"data\")\n",
    "\n",
    "# 해당 값은 리스트 형태로 제공되기에 리스트만큼 반복\n",
    "for r in ranks:\n",
    "    # 각 데이터는 rank, keyword, keyword_synomyms\n",
    "    rank = r.get(\"rank\")\n",
    "    keyword = r.get(\"keyword\")\n",
    "    print(rank, keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-knight",
   "metadata": {},
   "source": [
    "## 또는 아래 링크를 참고:\n",
    "\n",
    "https://jaeho0613.tistory.com/112\n",
    "\n",
    "여기보면 크롤링을 네이버에서 직접하지 않고, datalab에서 한다.\n",
    "크롤링이 막힐 경우 유저설정을 한다 (사람임을 알림)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inaction",
   "language": "python",
   "name": "inaction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
